# Phase 1: Research Instruments & Protocols

**Feature**: agentii-kit marketplace platform
**Branch**: `001-kit-marketplace`
**Created**: 2025-12-04
**Purpose**: Detailed protocols, guides, and instruments for Phase 1 research (Problem & Persona Validation)

---

## Interview Protocol 1: Domain Expert Kit Creator

**Target**: 12-15 domain experts (Product Managers, Legal Counsel, Marketing Directors, Financial Analysts, Educators) with 5+ years experience

**Duration**: 60 minutes

**Recruitment**: LinkedIn outreach, community forums (ProductTank, r/legal, marketing Slack communities)

**Screening Criteria**:
- 5+ years professional experience in target domain
- Has created or attempted to share processes/frameworks/templates with others
- Familiar with GitHub or willing to learn
- English-speaking

### Interview Schedule

| Time | Section | Duration |
|------|---------|----------|
| 0:00-0:05 | Warm-up & Context | 5 min |
| 0:05-0:20 | Current Knowledge-Sharing Pain | 15 min |
| 0:20-0:35 | JTBD Deep Dive | 15 min |
| 0:35-0:50 | Motivation & Monetization | 15 min |
| 0:50-1:00 | Kit Concept Reaction + Q&A | 10 min |

### Section 1: Warm-Up & Context (5 min)

**Goal**: Build rapport; establish credibility

**Script**:

> "Thanks for taking time today! I'm learning about how domain experts like you share knowledge and frameworks with your teams and communities. I'm working on a platform that might make that easier.
>
> Before we dive in, I'd like to understand your background a bit. Can you tell me about your current role and how long you've been in [Domain]?"

**Probes**:
- What's your title and company?
- How many years in this domain?
- What's your typical role re: knowledge sharing?

---

### Section 2: Current Knowledge-Sharing Pain (15 min)

**Goal**: Validate H1.1 - Acute pain with current workflows

**Core Question**:

> "Walk me through the last time you tried to share a process, framework, or checklist with others—either your team or the broader community. What did you want to accomplish?"

**Probe Follow-ups**:
- What medium did you use? (Notion, email, PDF, Gumroad, GitHub, Medium, etc.)
- Who was the audience?
- What went well? What was frustrating?
- How long did it take?

**Transition**:

> "That's really helpful. Let me ask: how often do you *feel the need* to share or codify something like that?"

**Frequency Probes**:
- Daily / Weekly / Monthly / Quarterly / Annually?
- What triggers the impulse to share?
- What prevents you from sharing more often?

**Pain Validation**:

> "On a scale of 1-10, where 10 is 'extremely frustrated with current options' and 1 is 'not frustrated at all,' how frustrated are you with how you currently share knowledge?"

**Workaround Probe**:

> "What's your current workaround for sharing? Do you have a favorite tool or approach?"

---

### Section 3: JTBD Deep Dive (15 min)

**Goal**: Validate H1.2 - Motivation is recognition, not revenue; validate JTBD specifics

**Core Question**:

> "If you could design the *ideal* way to share your expertise, what would that look like? Don't worry about technical constraints—just describe your ideal scenario."

**Probe Follow-ups**:
- Who would you want to reach?
- What would success look like for you?
- Would you want updates when people use your framework?
- Would you want to know who found it valuable?

**Recognition Motivation** (to validate H1.2):

> "Imagine two scenarios:
>
> **Scenario A**: You publish a framework, and 100 people use it, but you don't know who, and you can't tell if it helped. You'd get paid $1000.
>
> **Scenario B**: You publish a framework, you can see that 50 people use it, get direct feedback from them, see them contribute improvements, cite you, and you make $0.
>
> Which scenario appeals to you more?"

**Follow-up**:
- Why did you choose that one?
- What's important to you: audience reach, recognition, impact, revenue, or something else?

---

### Section 4: Monetization (10 min)

**Goal**: Validate creator monetization hypothesis for business model

**Core Question**:

> "Would you be interested in publishing on a platform where you could monetize your kit—e.g., charge $9-29/month for access, or offer it free and take donations?"

**Probe**:
- Which model appeals to you?
- What price would you set for your kit?
- What if the platform kept 30% (like Gumroad), vs. 0% (pure open-source)?

**Open-Source Appeal**:

> "What if instead, the platform was fully open-source and free, but kits could get viral—maybe trending on Product Hunt, GitHub stars, featured in newsletters? Would that appeal to you?"

**Follow-up**:
- Would you publish a kit on an open-source platform if visibility was high and contributors improved it?
- What would motivate you most: earning money, recognition, impact, community contribution, or a mix?

---

### Section 5: Kit Concept Reaction (10 min)

**Goal**: Gauge reaction to agentii-kit concept; surface any friction

**Concept Pitch**:

> "Here's what we're building: agentii-kit is a GitHub-like marketplace where domain experts publish 'job kits'—reusable templates and workflows for their field. A PM might publish a 'Product Launch Kit'; a lawyer might publish a 'Contract Negotiation Kit'; a teacher might publish a 'Lesson Planning Kit.'
>
> Each kit has:
> - A constitution (core principles for the domain)
> - Templates for the workflow
> - Examples
> - Versioning & collaboration built-in
>
> Junior PMs, paralegals, accountants, teachers can fork a kit, customize it for their context, and run it with their AI agent (Claude Code, Cursor). They save 10+ hours per project.
>
> You (the expert) get to see who's using your kit, get contributions/improvements back from the community, and become the recognized expert in your field.
>
> What's your gut reaction?"

**Follow-ups**:
- Would you publish a kit there?
- What would make this appealing to you?
- What concerns do you have?
- What would you want to see before trying it?

---

### Interview Protocol 2: Professional Practitioner Kit User

**Target**: 12-15 practitioners (Junior PMs, Paralegals, Content Marketers, Accountants, Teachers)

**Duration**: 45 minutes

**Recruitment**: LinkedIn, Reddit communities (r/productivity, r/startups), career Discord servers

**Screening Criteria**:
- 1-3 years in current role (junior-level)
- Repeats similar processes/workflows regularly
- Familiar with GitHub or willing to learn
- Has used AI agents (Claude Code, Cursor) or open to trying
- English-speaking

### Interview Schedule

| Time | Section | Duration |
|------|---------|----------|
| 0:00-0:05 | Warm-up & Context | 5 min |
| 0:05-0:18 | Current Process & Pain | 13 min |
| 0:18-0:30 | JTBD & Workaround Discovery | 12 min |
| 0:30-0:42 | Expert Kit Appeal & Monetization | 12 min |
| 0:42-0:45 | Concept Reaction + Close | 3 min |

### Section 1: Warm-Up & Context (5 min)

**Script**:

> "Thanks for taking time! I'm talking to professionals about how you find and apply best practices in your work. I'm exploring a platform to make that easier.
>
> First, tell me about your current role. What do you do day-to-day?"

**Probes**:
- What's your title and company?
- What's a typical project or workflow?

---

### Section 2: Current Process & Pain (13 min)

**Core Question**:

> "Think about a recent project where you had to execute a process or workflow—like launching a feature, negotiating a contract, planning a campaign. Walk me through how you approached it. Where did you look for guidance?"

**Probe Follow-ups**:
- Did you ask a colleague?
- Did you Google best practices?
- Did you look for templates online?
- What did you struggle with?

**Time/Waste Probing**:

> "Roughly how many hours did that project take? How much time do you think you wasted because you didn't know the best approach, or had to invent it from scratch?"

**Frequency**:

> "How often do you face similar projects or workflows? Do you repeat similar processes regularly?"

---

### Section 3: JTBD & Workaround Discovery (12 min)

**Core Question**:

> "If a senior expert in your field handed you a step-by-step playbook for [typical workflow for their role] that they'd tested 100 times, and you could just follow it, what would that be worth to you? Time saved? Better quality?"

**Probe**:
- How many hours would you save per project?
- How would the quality of your work change?
- Would you be able to do it independently, or would you still need help?

**Willingness to Adopt**:

> "If such a playbook or template existed, would you use it?"

**Willingness to Pay**:

> "Would you use it if it were free? Free but you had to sign up for an account? Free but with premium features you could buy? Or would you not use it at all?"

---

### Section 4: Expert Kit Appeal (12 min)

**Concept Introduction**:

> "Here's what I'm exploring: agentii-kit is a marketplace where domain experts publish 'job kits'—templates and playbooks for their field, maintained by the expert.
>
> A kit includes:
> - Step-by-step workflow
> - Templates you customize for your project
> - Examples
> - It integrates with AI agents (Claude Code, Cursor)
>
> You'd fork the kit, fill in your context, and the AI would help you execute it. You'd save time and get better quality.
>
> The experts maintain the kits; they get recognized and see who's using their kit.
>
> What do you think?"

**Follow-ups**:
- Would you use a kit like this?
- For which types of work?
- What would make you trust the kit?
- Would you want to be able to customize it?
- Would you want to contribute improvements back?

---

### Section 5: Concept Reaction & Close (3 min)

**Quick Reaction**:

> "On a scale of 1-10, how likely would you be to try agentii-kit if it were available today?"

**Follow-up if <7**:
- What would make it a 9 or 10?

**Close**:

> "You've been super helpful. Before I let you go—is there anyone else in your field I should talk to?"

---

## Usability Test Protocol 1: Kit Creator Hero Workflow

**Objective**: Validate H1.3 - Can domain experts publish a basic kit in <30 minutes?

**Target**: N=5-8 domain experts (can use some from interviews if they volunteer, or recruit separately)

**Setup**: Moderated session, ideally in-person or Zoom with screen sharing

**Duration**: 45 minutes (30 min task + 15 min debrief)

### Pre-Test

**Materials Provided**:
- Kit template scaffold (.zip file with structure)
- One-page "Quick Start" guide (500 words max)
- Sample kit for reference (E.g., "SEO-Kit" example)
- Access to agentii-kit documentation

**Instructions**:

> "Your task: In the next 30 minutes, create a basic kit for your domain. Use the templates provided. Include:
>
> 1. Constitution.md (5-7 core principles for your domain)
> 2. Spec-template.md (what practitioners need to specify)
> 3. Plan-template.md (how to execute work in your domain)
> 4. One worked example showing the hero workflow
> 5. README explaining the kit
>
> Your goal is to create something publishable—not perfect, but complete. You have 30 minutes. Go!"

### During Test

**Facilitator Observation Focus**:
- Where do they start?
- What confuses them?
- Do they reference the sample kit?
- Do they re-read instructions?
- Where do they get stuck? (Constitution? Templates? Examples?)
- What's easy?
- Do they finish on time? If not, how close?

**Facilitator Role**: Observe silently; only help if they're completely blocked (e.g., "I don't know what constitution.md is")

### Post-Test Debrief (15 min)

**Questions**:

1. "Walk me through what you created. What were you thinking?"
2. "What was easy about this task?"
3. "What was hard? Where did you get stuck?"
4. "How would you rate the difficulty? (1=trivial, 5=challenging)"
5. "How would you rate the clarity of instructions? (1=very confusing, 5=crystal clear)"
6. "On a scale 1-5, how satisfied are you with what you created?"
7. "Would you publish this kit as-is?"
8. "What would make this task easier?"

**Measurement**:
- Time to completion
- Completeness (all 5 sections done?)
- Quality (is it publishable?)
- Satisfaction (1-5)
- Clarity of instructions (1-5)
- Errors / confusion points

---

## Usability Test Protocol 2: Kit User Hero Workflow

**Objective**: Validate H2.3 - Can practitioners discover, fork, customize, and execute in <15 minutes?

**Target**: N=8-10 practitioners (mix of roles: PM, Paralegal, Marketer, Accountant, Teacher)

**Setup**: Moderated session, ideally in-person or Zoom with screen sharing; they need Claude Code or Cursor

**Duration**: 35 minutes (15 min task + 20 min debrief)

### Pre-Test

**Setup**:
- Provide them with a problem scenario (role-specific)
- Show them the agentii-kit marketplace interface (prototype or mock if needed)
- Ensure they have Claude Code or Cursor installed
- Give them a forked kit in their workspace to customize

**Scenario Examples**:

| Role | Scenario |
|------|----------|
| Junior PM | "You're launching a product next month. You need a launch plan. Find a kit, fork it, customize it for your product, and run the workflow with your AI agent." |
| Paralegal | "You need to draft and review an NDA. Find a legal kit, customize it for your company, and get first-draft output from your AI agent." |
| Marketer | "You're running a Q4 campaign. Find a marketing kit, customize for your campaign, and generate a content calendar using your AI agent." |

### During Test

**Facilitator Observation Focus**:
- Can they find the kit? (Discovery friction)
- Do they understand how to fork?
- Do they know how to customize spec.md?
- Can they trigger the workflow in Claude Code / Cursor?
- Where do they get stuck?
- Do they generate useful output?
- Time spent on each phase (discovery, fork, customize, execute)

**Facilitator Role**: Observe silently; help only if completely blocked

### Post-Test Debrief (20 min)

**Questions**:

1. "Walk me through what you did. What was your thinking at each step?"
2. "Did you successfully generate output from the kit?"
3. "On a scale 1-5, how useful was that output?"
4. "What was easy?"
5. "What was hard? Where did you get stuck?"
6. "How would you rate the overall experience? (1=frustrated, 5=delighted)"
7. "If you had more time, would you run more tasks from this kit?"
8. "Would you use this kit again for future projects?"
9. "What would make this experience better?"

**Measurement**:
- Time to completion
- Success (did they generate output?)
- Output quality (useful? on-brand? complete?)
- Satisfaction (1-5)
- Net Promoter Score: "Would you recommend this to a colleague?" (1-10)
- Friction points (discovery, fork, customize, agent integration)

---

## Survey Template: Template Marketplace User Research

**Objective**: Validate H3.1 - Demand for template marketplaces is real

**Target**: N=100-200 existing template users (Notion, Gumroad, Zapier, etc.)

**Distribution**: Notion template marketplace, Gumroad communities, Zapier forums

**Format**: 10-12 questions, ~3 minutes

### Questions

**Screening** (Q1-2):

1. Have you purchased or used a template from a marketplace like Notion, Gumroad, or Zapier in the past 12 months?
   - Yes / No

2. What type of templates do you typically use? (Check all that apply)
   - Project management / productivity
   - Business / legal
   - Marketing / sales
   - Finance / accounting
   - Other: ___

**Current Workflow** (Q3-5):

3. Where do you typically find templates? (Rank top 3)
   - Google search
   - Notion / Gumroad / Zapier marketplace
   - Reddit / Twitter recommendations
   - Friend / colleague recommendation
   - Other: ___

4. What's your biggest frustration with current template marketplaces? (Open-ended, 50 words)

5. On a scale 1-5, how easy is it to find templates that match your specific needs?

**Willingness to Switch** (Q6-8):

6. If a platform offered:
   - Better discoverability (search, tags, ratings)
   - Open-source templates (free, collaborative)
   - Integration with AI agents
   - Community contributions & improvements
   - Would you be interested in trying it?
     - Very interested / Somewhat interested / Not interested

7. Which would you prefer?
   - Proprietary templates (paid, no modifications allowed)
   - Open-source templates (free, can modify & contribute)

8. Would you be willing to use a GitHub-based template marketplace (vs. web UI)?
   - Yes, very comfortable with GitHub
   - Yes, but I'd need onboarding
   - No, too technical

**Demographics** (Q9-12):

9. What's your role?
   - PM / Product Designer
   - Marketer / Sales
   - Legal / Compliance
   - Finance / Operations
   - Educator / Academic
   - Other: ___

10. What AI agent(s) do you use, if any?
    - Claude Code
    - Cursor
    - GitHub Copilot
    - Gemini
    - None / Not sure
    - Other: ___

11. Company size?
    - Startup (1-50)
    - Small (51-200)
    - Mid-market (201-1000)
    - Enterprise (1000+)

12. Interest in beta access to agentii-kit?
    - Yes, email me
    - Maybe, tell me more
    - Not interested

---

## Research Tracking & Reporting

### Data Collection Sheet

| Interview ID | Date | Persona | Duration | Key Insight | Pain Level (1-5) | JTBD Validated? | Notes |
|--------------|------|---------|----------|-------------|------------------|-----------------|-------|
| C1 | 2025-12-10 | PM | 62 min | Wants recognition more than revenue | 4 | Yes | Ready to publish on day-1 launch |
| C2 | 2025-12-10 | Legal | 58 min | Frustrated with Gumroad discovery | 5 | Yes | Willing to co-create legal-kit |
| … | … | … | … | … | … | … | … |

### Analysis Checklist

- [ ] All interviews recorded and transcribed
- [ ] Thematic analysis: Group insights by JTBD and hypothesis
- [ ] Saturation check: Are new interviews repeating insights or adding new themes?
- [ ] Go/No-Go scoring: Do we meet success thresholds for each hypothesis?
- [ ] Pivot signals: Any conflicting data or unexpected personas?
- [ ] Report prepared for decision-maker review

---

## Next Steps

→ After completing Phase 1 research instruments, proceed to create:
- Validation checkpoints & go/no-go decision matrix (validation-checkpoints.md)
- Agent context update with research findings
- Executive summary for `/pmfkit.implement` launch planning
