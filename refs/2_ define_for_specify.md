> **Author:** frank@agentii.ai  
> **Version:** v0.1.0  
> **Last Updated (NYC):** 2025-12-03 04:34

# step 2: define critical information for your project

This step corresponds to the **spec.md / `/speckit.specify`** phase in Spec Kit, but focused on **PMF for an AI/LLM SaaS product**, not on implementation or technology.

At this stage, you should define:

- Who the product is for (personas and segments)
- What core problems and jobs-to-be-done it solves
- What the initial product scope and “hero workflows” are
- How you will recognize product-market fit (behavioral and business signals)
- What constraints, risks, and open questions shape the project

No tech stack or architecture details should appear here.

---

## 1. Project summary

- **Working name:** (to be filled by founder / team)
- **Product type:** AI/LLM-powered SaaS / agent product
- **Domain:** (e.g., developer tools, creative tools, vertical/enterprise, productivity)
- **Primary goal:** Achieve strong, repeatable **product-market fit (PMF)** for a narrowly defined use case in the chosen domain.

## 2. Target users & personas

Describe 1–2 *primary* personas you care about for the first PMF push.

- **Primary persona:**  
  - Role and seniority (e.g., senior backend engineer, video creator, in-house counsel, product marketer)  
  - Company size and context (indie, startup, SMB, mid-market, enterprise team)  
  - Daily tools and environment (IDE, design tools, office suite, communication tools, project management tools)

- **Secondary persona (optional):**  
  - If there is a clear secondary user (e.g., reviewer/manager, collaborator), describe who they are and how they interact with the product.

## 3. Core problems and jobs-to-be-done

For the primary persona:

- **Top problems today:**  
  - What work is slow, tedious, expensive, or error-prone?  
  - Where do they rely on multiple tools or manual glue work?  
  - What do they avoid doing because it’s too painful?

- **Jobs-to-be-done (JTBD):**  
  For each problem, write 1–3 JTBD statements of the form:  
  - "When [situation], I want to [job], so I can [outcome]."

- **Existing alternatives:**  
  - What tools and workflows do they use today (including non-AI tools, agencies, spreadsheets, scripts, or nothing)?

## 4. Initial product scope and hero workflows

Define the **first wedge** and **hero workflows** that the product will support. These should be **narrow and high-value**, similar in spirit to how:

- Cursor / Claude Code / Devin focus on specific coding loops
- Runway / Pika / HeyGen focus on specific video creation/editing flows
- Harvey / Writer focus on specific legal or enterprise content tasks
- Canva / Figma / Notion focus on specific creation and collaboration patterns

Specify:

- **Single wedge use case:**  
  - One clearly defined use case where the product aims to become the *default* tool for the primary persona.

- **Hero workflow (end-to-end):**  
  - 5–8 concrete steps from user intent → AI-assisted work → finished artifact or outcome.  
  - What inputs the user provides (docs, data, briefs, repositories, videos, etc.).  
  - What kind of outputs they expect (PRs, documents, videos, designs, reports, decisions).

- **Out-of-scope (for now):**  
  - Adjacent use cases and personas you are *deliberately not* targeting in the first PMF phase.

## 5. Desired user outcomes and value

Describe what “10x better” looks like for the primary persona:

- **Functional outcome:**  
  - What can they now do that they could not do before?  
  - Which workflows become realistically possible for smaller teams or individuals?

- **Efficiency outcome:**  
  - Time savings on key tasks or projects.  
  - Reduction in external spend (agencies, contractors, tools) if applicable.

- **Quality outcome:**  
  - Fewer errors/bugs/mistakes.  
  - More consistent outputs, better adherence to brand/standards/policies.

- **Emotional outcome:**  
  - How does the product change how users *feel* about this part of their job (e.g., from anxiety to confidence, from boring to creative)?

## 6. PMF signals and success metrics (non-technical)

Define how you will recognize early PMF signals for this AI SaaS product. Focus on **user behavior and business outcomes**, not models or infra.

- **Engagement & retention:**  
  - Expected usage frequency (daily, weekly, per project).  
  - Target retention curves or minimum acceptable retention (e.g., % of users still active after 1/4/12 weeks).

- **Depth of usage:**  
  - How often users complete the hero workflow.  
  - Whether users keep the product open during their core work sessions.

- **Value and willingness to pay:**  
  - Signals that users or teams are willing to pay or expand usage (e.g., upgrades, adding teammates, moving real projects into the product).

- **Qualitative PMF signals:**  
  - “Very disappointed if this product went away” responses.  
  - Quotes that show the product has become the default way they do the job.

## 7. Constraints, risks, and assumptions

List the non-technical factors that shape how you approach PMF:

- **Market & segment constraints:**  
  - Budget levels, procurement friction, competition, regulatory environment.

- **Adoption constraints:**  
  - Need for trust, governance, approvals, or legal/compliance sign-off.  
  - Change management in teams already using tools like Canva, Figma, Notion, existing IDEs, or legacy processes.

- **Assumptions to validate:**  
  - Assumptions about user behavior, appetite for AI automation/agents, and switching costs.  
  - Assumptions about how quickly you can iterate with real users.

- **Key risks:**  
  - Being a thin wrapper around foundation models (no workflow or data moat).  
  - Over-broad scope and weak PMF for any one use case.  
  - “Autonomy theater” that overpromises what the AI/agent can do.

## 8. Open questions

Capture the biggest unknowns you want to investigate next, for example:

- Which exact persona and segment shows the strongest early pull?  
- Which hero workflow is the best starting wedge?  
- What pricing and packaging will align with perceived value and budget?  
- Which channels (community, ecosystem, integrations) are likely to drive early distribution?

These questions will drive research, user interviews, and experiments in later steps.

---

## Perplexity research prompt

Use the prompt below in Perplexity (or similar tools) to find **recent case studies, playbooks, and best practices** that can validate and refine this PMF-focused spec.

```prompt
you are an expert product strategist and ai saas operator working in 2024–2025.

based on the following high-level spec for an ai/llm saas product that is trying to achieve strong product-market fit (pmf):

- focus on a narrow, high-value job-to-be-done for a specific primary persona and segment
- define one or two "hero workflows" that go from user intent to finished artifacts/outcomes
- measure pmf with behavioral and business signals (retention, depth of usage, willingness to pay, qualitative love)
- avoid early technical detail; emphasize users, problems, jobs-to-be-done, value, constraints, and risks

extensively search for **recent (roughly 2023–2025)** case studies, interviews, and analyses of successful ai/llm-based saas and agent products with strong pmf, especially:

- developer / agent tools: cursor, claude code, devin, manus, lovable.dev
- creative / media tools: runway, pika, heygen, descript, photoroom
- vertical / enterprise tools: harvey (law ai), writer.ai, and similar vertical copilots
- iconic pmf examples: canva, figma, notion, and other adjacent tools

for each relevant example you find, extract and summarize:

1. which persona and segment they initially focused on
2. the core problems and jobs-to-be-done they solved first
3. the initial wedge use case and hero workflows
4. how they measured and recognized early pmf (quantitative and qualitative signals)
5. key constraints, risks, and strategic choices that shaped their path to pmf

then, propose improvements or additions to my pmf spec structure (personas, problems, jobs-to-be-done, hero workflows, success metrics, constraints, open questions) so that it better reflects the practices of these successful products.

provide a clear, structured markdown answer with:

- a short summary of cross-cutting patterns
- a table mapping each product to the 5 items above
- concrete suggestions for how to refine my spec in "step 2: define critical information for your project" so it aligns with modern pmf best practices for ai saas.

```



---

# Principles of Product-Market Fit (PMF) for AI SaaS Products (2023–2025 Patterns)

## Short summary of cross-cutting patterns

Across modern AI/LLM SaaS and agent products (Cursor, Claude Code, Devin, Runway, Pika, HeyGen, PhotoRoom, Harvey, Writer, Canva, Figma, Notion, etc.), the strongest PMF stories share the same structural moves:

- Start with a very specific persona and job-to-be-done (JTBD), not “everyone who uses AI.”
- Anchor on one or two “hero workflows” that go from user intent to finished artifact (code change, exported video, signed-ready contract, production-ready asset).
- Make those workflows AI-native, opinionated, and fast, with strong guardrails and human control.
- Measure PMF via depth of workflow usage, retention, and willingness to pay, not launch hype or signups.
- Treat constraints (trust/safety, latency, integrations, compliance) as design inputs, not afterthoughts.
- Use artifacts, templates, and integrations to drive PLG growth and expansion to teams.

Your PMF spec already captures some of this. The improvements mainly involve: being more explicit about personas, workflows, and data/constraints; including AI-specific metrics; and adding “distribution & adoption” plus “trust/safety” explicitly into the spec.

***

## Product-by-product PMF patterns (2023–2025)

Below is a high-level synthesis for each example product.

### Mapping table

| Product           | 1. Initial persona / segment                               | 2. Core early problems & JTBD                                                                 | 3. Initial wedge & hero workflows                                                                                                     | 4. Early PMF signals (quant + qual)                                                                                           | 5. Key constraints, risks, and strategic choices                                                                 |
|------------------|-------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|
| Cursor           | Professional software engineers; early on, heavy AI-curious devs using VS Code/JetBrains-style flows | - Writing and editing code faster in IDE  - Understanding and modifying large codebases  - Reducing friction for common dev tasks | - Wedge: “AI-first code editor” rather than plugin  - Hero workflows: inline AI edits, multi-file refactor, “implement X feature” from natural language | - Explosive usage: devs living in Cursor all day, replacing their main editor  - Rapid ARR growth from individual dev plans  - Strong social proof and word-of-mouth from developer communities | - Needed deep IDE-level integration and low-latency model experience  - Chose to build a full editor (strategic complexity)  - Invested early in custom models and context handling |
| Claude Code      | Developers in orgs already using Claude; internal Anthropic teams, then external devs | - Need for AI help with debugging, refactoring, code explanations  - Navigating large repos via natural language                   | - Wedge: “coding mode” for Claude with repo understanding  - Hero workflows: ask about code, generate patches, walk through fixes                      | - Increased internal dev productivity (Anthropic dogfooding)  - High usage of coding-specific mode vs general chat  - Positive qualitative feedback on code reliability | - Must avoid bad code suggestions in safety-critical contexts  - Strategic choice to keep UX inside a chat-like experience while adding structure |
| Devin (Cognition) | Early adopters: startups and teams wanting end-to-end coding assistance; AI-forward dev teams | - Offloading scoped dev tasks end-to-end (ticket → code → tests → deploy)  - Reducing coordination overhead on repetitive tasks | - Wedge: “AI software engineer” executing tasks in a controlled environment  - Hero workflows: take a ticket, edit repo, run tests, open PR             | - High inbound interest and waitlist  - Observed ability to complete real tasks in constrained environments  - Social proof via demos and case studies | - Huge risk of “autonomy theater” vs real reliability  - Chose constrained environments and live demos of real tasks  - Heavy emphasis on observability and sandboxing |
| Lovable.dev      | Indie founders, small dev teams building MVPs                | - Turning product ideas into working web apps quickly  - Handling repetitive boilerplate and scaffolding                           | - Wedge: “build a full app from a prompt/repo”  - Hero workflows: describe MVP → generate app → modify iteratively in the same environment             | - Users shipping real side projects and MVPs  - High viral sharing of “I built X in a weekend” stories  - Repeat builds by same users                 | - Risk of generating unmaintainable code  - Chose to focus on MVP/early-stage rather than mission-critical systems  - Trade-off between flexibility and guardrails |
| Runway           | Professional and serious amateur video creators, studios, agencies | - Time-consuming, expensive post-production (keying, rotoscoping, VFX)  - Need for more expressive generative video              | - Wedge: AI tools around video editing (inpainting, background removal), then Gen-1/Gen-2 generative video  - Hero workflows: text-to-video scenes, video editing and style transfer | - Growing number of films, ads, music videos using Runway  - Willingness to pay for credits and pro tiers  - Success stories showcased in marketing | - Heavy GPU cost + latency  - Must meet quality standards of production workflows  - Chose to stay focused on video instead of general media |
| Pika             | Short-form content creators; social-first creators on X/TikTok; indie animators | - Quickly producing stylized, short, eye-catching clips  - Prototyping video ideas without full production stack                  | - Wedge: easy text-to-video and video editing in browser/Discord  - Hero workflows: “make 3–10 second stylized clip from prompt/reference”            | - Viral usage and social sharing of Pika-generated clips  - Strong daily/weekly usage by creators  - Fast growth of community and templates           | - High expectations for visual quality and style control  - Strategic focus on short-form vs long-form; stylized vs fully realistic |
| HeyGen           | Marketers, L&D teams, SMBs needing talking-head & explainer videos | - Creating professional spokesperson videos without cameras/actors  - Localizing content across languages                         | - Wedge: template-based talking-head avatar videos  - Hero workflows: pick avatar/template → input script → generate multi-language videos            | - High product usage by thousands of businesses  - Willingness to pay for higher-res and more avatars  - Viral share of explainer and localization use cases | - Risk of uncanny valley and brand risk  - Compliance and consent for likeness and voices  - Strategic choice to focus on business use vs entertainment |
| PhotoRoom        | Online sellers, SMBs, social sellers, marketplaces           | - Need for clean, consistent product photos without pro photography  - Fast creation of on-brand visuals                          | - Wedge: background removal for product photos  - Hero workflows: upload product image → 1-click background removal → apply templates → export       | - Huge daily usage; users replace manual tools/Photoshop for many tasks  - Paid adoption by serious sellers  - Strong mobile engagement and retention | - Must balance quality and speed on mobile  - Risk of being seen as “just a filter app” → countered by expanding into templates/brand kits |
| Descript         | Podcasters, video editors, marketing teams                    | - Editing audio/video is slow and technical  - Need for text-based editing, overdubs, fillers removal                              | - Wedge: text-based podcast editing  - Hero workflows: transcribe → edit text → audio/video updates automatically                                     | - Users editing entire episodes in Descript  - Willingness to pay for teams, higher limits  - Creators evangelizing tool to peers                      | - Voice-cloning and deepfake concerns  - Positioning as pro tool vs beginner tool  - Strategic shift from just podcasters to broader video |
| Harvey (law AI)  | Law firms and in-house legal teams                            | - Drafting and reviewing contracts, memos, and research is time-consuming and repetitive  - Need safe summarization and precedent use | - Wedge: contract review, drafting, and summarization for specific practice areas  - Hero workflows: upload corpus → query/draft standard docs → redline and review | - Won marquee firms and scaled within them  - Lawyers using it on real matters, not just experiments  - Strong willingness to pay for efficiency and competitive edge | - Extreme accuracy, confidentiality, and auditability required  - Deep integration with firm knowledge bases and workflows  - Focus on a small number of high-value tasks first |
| Writer.ai        | Enterprise marketing, support, operations teams               | - Inconsistent brand tone and terminology  - Slow content production cycles  - Need for safe, governed generative AI               | - Wedge: brand-aligned content generation tied to style guides and knowledge  - Hero workflows: draft content from templates, rewrite into brand voice, QA against terminology and policies | - Multi-team deployments within enterprises  - Expansion from a single team to many departments  - Buyers citing governance and control as main reasons | - Must integrate with existing content/knowledge systems  - Strong governance and security requirements  - Chose verticalized copilot patterns (support, sales, HR, etc.) |
| Canva            | Non-designers in SMBs, marketing, education                   | - Need to create decent designs quickly  - Reliance on expensive designers or poor-quality DIY graphics                            | - Wedge: easy online design with templates  - Hero workflows: social media post/flyer/presentation from template with drag-and-drop editing          | - Massive daily active usage and design creation volume  - Viral spread via shared templates and exports  - Strong subscriptions and team adoption      | - Trade-off between simplicity and power  - Heavy investment in template ecosystem and brand kits  - Later added AI as accelerant, not core wedge |
| Figma            | Product designers, design teams, then adjacent roles          | - Collaborative UI design across distributed teams  - Breaking file-based workflows (Sketch, static handoffs)                      | - Wedge: collaborative interface design in browser  - Hero workflows: multi-player design and prototyping for app/web UI                             | - Entire product teams adopting Figma as default  - High time-in-product and cross-role engagement  - Strong expansion from teams to orgs              | - Performance and reliability for heavy files  - Strategic focus on professional designers first, then developers/PMs  - Later added AI features |
| Notion           | Startup teams, knowledge workers, product/ops teams           | - Fragmented docs, notes, and tasks across tools  - Need flexible, simple knowledge/workspace                                     | - Wedge: all-in-one doc/wiki with databases  - Hero workflows: team wiki, project notes, lightweight CRM                                             | - Teams adopting Notion as “home base”  - Templates and shared pages driving adoption  - PLG motion from individuals to teams and companies           | - Ambiguity between personal and team use  - Balanced flexibility vs structure  - Later layered AI on top of established workflows |

(Note: Manus and some smaller tools have less public detail, but they generally follow similar patterns as Lovable/Cursor-style coding agents.)

***

## How to improve your PMF spec structure

Your current spec includes:

- Narrow, high-value JTBD for a specific persona
- 1–2 hero workflows from intent to outcome
- PMF measured by retention/usage/willingness to pay/qualitative love
- De-emphasis of early technical detail

That’s a good starting point. To better align with modern PMF practice from these products, refine “Step 2: Define critical information for your project” with the following additions and clarifications.

### 1. Personas & Segments: Make them sharper and more “real”

Add explicit fields:

- Primary persona:
  - Role/title and skill level (e.g., “Senior backend engineer at a 50–500-person SaaS company,” “solo Shopify seller,” “associate at a BigLaw firm”).
  - Context: tools they already live in, environment (web/desktop/mobile), and constraints (compliance, budgets).

- Initial segment:
  - Company size / vertical / geography (e.g., “US-based Series A–C SaaS companies,” “European DTC brands,” “AM Law 100 firms”).
  - Why this segment is ideal early (pain intensity, access, willingness to experiment, high reference value).

Template snippet you can add:

- Persona:  
  - Name + role:  
  - Skill level:  
  - Current tool stack:  
  - Environment and constraints:  

- Initial segment:  
  - Company size / type:  
  - Geography / industry:  
  - Why they are the right early market:  

### 2. Problems & JTBD: Anchor in real workflows, not abstract pain

Instead of only listing “pain points,” structure them as JTBD that mimic the successful products:

- Job stories:
  - “When I [situation], I want to [job], so I can [desired outcome].”

- Explicit “current workaround”:
  - How they do this today (tools, hacks, manual steps).
  - What they dislike about their current approach (time, cost, risk, quality).

Add fields:

- Top 3 jobs-to-be-done (ranked):
  1. Job story + current workaround + failure criteria.
  2. Job story + current workaround + failure criteria.
  3. Job story + current workaround + failure criteria.

- “10x better than today” definition:
  - For each JTBD, define what 10x improvement looks like (e.g., time cut by 80%, error rate halved, no need for external contractor, etc.).

### 3. Hero Workflows: Explicit, step-by-step, AI-native

From case studies, PMF is built around highly specific hero workflows. Upgrade your spec to force that level of specificity:

For each hero workflow:

- Name:
  - Example: “Create Amazon-ready product photo,” “Refactor feature X in repo,” “Draft NDA from playbook.”

- Trigger/intention:
  - What the user is trying to accomplish when they start (ticket, brief, asset, document, etc.).

- Inputs:
  - Concrete inputs: files, URLs, text, existing systems (repo, DMS, CMS).

- Steps:
  - Step 0: How the user discovers/starts this workflow in your product.
  - Step 1–N: Major steps, with notes on:
    - What the AI does.
    - What the user does (review, approvals, adjustments).
    - Where guardrails or constraints appear (e.g., diff views, legal disclaimers).

- Output:
  - Final artifact (merge request, exported video, signed-and-ready doc).

- Success criteria:
  - What must be true for the user to consider it a “success” (usable without major rework; passes quality bar; integrated in their system).

- “Time-to-first-wow” target:
  - Maximum minutes from first signup to completing this workflow once.
  - Example: “New user should complete this workflow in under 10 minutes with their own data.”

Adding a “Hero Workflow Spec” block:

- Hero workflow name:  
- Persona & JTBD it serves:  
- Trigger:  
- Inputs:  
- Steps (user vs AI responsibilities):  
- Guardrails & approvals:  
- Output and where it goes:  
- TTFW target (min):  
- Minimum quality bar:  

### 4. PMF Metrics: Add AI-specific and workflow-level measures

Your spec already mentions retention, usage depth, and willingness to pay. Enhance it with:

- Workflow-level behavioral metrics:
  - % of new users who complete hero workflow at least once in first week.
  - Median time-to-first-wow for hero workflow.
  - Median number of times per week/month an active user runs hero workflow.

- AI-specific quality metrics:
  - Task completion rate (per workflow).
  - Edit distance / human correction effort:
    - For text/code: average edits per output, or time spent editing.
    - For media: re-generation count, manual overrides.
  - Prompt-to-output time (median and p90).
  - Autonomy rate (where applicable) with boundaries.

- Business metrics:
  - Conversion from active free users (who completed hero workflow) to paid.
  - Net revenue retention (once you have teams).
  - Seat/usage expansion: average number of seats or usage units per account over time.

Add a “Success Metrics” section per hero workflow:

- Activation:
  - % of signups who complete hero workflow in 7 days.
  - TTFW (median, p90).

- Engagement:
  - Avg. successful runs per active user per week/month.
  - Retention at 4/12 weeks for users who completed hero workflow.

- Quality:
  - Task completion rate (%).
  - Edit effort (qualitative description + any quantitative proxies).
  - Error/hallucination rate (if applicable).

- Business:
  - Conversion to paid for users who completed hero workflow.
  - Average revenue per account linked to this workflow (if separable).

### 5. Constraints & Risks: Make them a first-class part of the spec

Case studies show constraints drove key strategic choices (e.g., Harvey’s focus on confidentiality, Cursor’s need for low latency, Runway’s GPU cost). Add structured sections:

- Domain & compliance constraints:
  - Regulatory requirements (legal, healthcare, finance).
  - Data privacy and residency needs.
  - Content safety constraints (e.g., likeness rights for avatars).

- Technical and UX constraints:
  - Acceptable latency for each workflow.
  - Required integrations for the workflow to be useful (GitHub, Figma, CMS, DMS, etc.).
  - Device environments (desktop, mobile, web only).

- Business constraints:
  - Pricing expectations in early market.
  - Required unit economics (e.g., GPU cost vs ARPU).
  - GTM constraints (self-serve vs sales-assisted, need for references).

- Risk register (explicit):
  - Top 3–5 risks:
    - Example: “Autonomy theater risk: overpromising fully autonomous agent when real reliability is narrow.”
    - “Wrapper risk: being indistinguishable from base models.”
    - “Trust risk: hallucinations in high-stakes outputs.”
  - For each risk: likelihood, impact, mitigation plan.

### 6. Distribution & Adoption: Add explicit growth hypotheses

Your spec currently emphasizes PMF, not GTM. However, in practice, PLG and distribution shape the product. Add:

- Entry motion:
  - How does the first persona discover and try the product? (community, templates, integrations, open source, content).

- Shareable artifacts:
  - What outputs can reasonably carry your brand in public (videos, screenshots, public docs, sharable templates, etc.)?

- Expansion motion:
  - How does a single user pull in teammates?
  - What collaboration or admin features will make teams adopt?

Add a “Distribution & Growth Patterns” subsection:

- First channel hypotheses (ranked):  
- Shareable outputs and how they show your brand:  
- Team expansion mechanism:  
- Land-and-expand path (individual → team → org):  

### 7. Open Questions: Make them explicit and tied to PMF risks

Finally, refine your “open questions” so they are directly tied to PMF:

- Persona fit questions:
  - “What specific subsets of [persona] show the strongest early pull and why?”

- Workflow questions:
  - “What part of the hero workflow users still do outside the product and why?”
  - “What edge cases break trust?”

- Value & willingness-to-pay questions:
  - “Which segment is willing to pay the most for this workflow?”
  - “What job or budget line item are we replacing (contractors, tools, internal headcount)?”

- Model & tech questions (kept high-level):
  - “Where does base model performance fall short of the quality bar for our persona?”
  - “What data or integration is missing for the workflow to feel ‘native’ to how they work today?”

***

## Suggested “Step 2: Define Critical Information” structure

Here is a concise structure that folds all of this into your spec, in line with modern AI SaaS PMF practice:

1. Personas & Segments  
   - Primary persona (role, skill, environment, tools).  
   - Initial segment (company type/size/vertical/geography, why they are ideal).  

2. Problems & Jobs-to-Be-Done  
   - Top 3 JTBD as job stories, each with:  
     - Current workaround and tools.  
     - Key frustrations.  
     - “10x better” definition.  

3. Hero Workflows (1–2 max)  
   For each workflow:  
   - Name and targeted JTBD.  
   - Trigger/intention.  
   - Inputs and integrations.  
   - Step-by-step (user vs AI responsibilities, guardrails).  
   - Output and where it goes.  
   - TTFW target and minimum quality bar.  

4. Success Metrics & PMF Signals  
   - Activation metrics (completion rate and time for hero workflows).  
   - Engagement metrics (frequency, cohorts).  
   - AI-specific metrics (completion, edit effort, latency, autonomy where relevant).  
   - Business metrics (conversion to paid, ARPU/NRR where applicable).  
   - Qualitative signals to watch for (“can’t live without it,” replacements, advocacy).  

5. Constraints & Risks  
   - Domain/compliance constraints.  
   - Technical/UX constraints (latency, device, integrations).  
   - Business constraints (pricing bands, unit economics).  
   - Top 3–5 PMF risks with mitigation hypotheses.  

6. Distribution & Adoption Hypotheses  
   - First acquisition channels (with persona-channel fit).  
   - Shareable artifacts & template strategy.  
   - Team expansion mechanisms and product features that support them.  

7. Open Questions  
   - Persona fit questions.  
   - Workflow depth and trust questions.  
   - Value and pricing questions.  
   - Model/data sufficiency questions (high-level).

Using this enriched structure will align your PMF spec much more closely with the concrete practices of the AI/LLM products that have demonstrated strong PMF in 2023–2025.

[1](https://www.productgrowth.blog/p/how-cursor-ai-hacked-growth)
[2](https://www.anthropic.com/news/introducing-claude)
[3](https://wandb.ai/byyoung3/ml-news/reports/Cognition-Labs-Unveils-Devin-A-New-AI-Software-Engineer--Vmlldzo3MTM5NDI5)
[4](https://gru.ai/blog/behind-cursors-success/)
[5](https://www.anthropic.com/news/how-anthropic-teams-use-claude-code?%3F__hstc=43401018.71aa366c60c32c7e3032e45be702fadd.1753488000253.1753488000254.1753488000255.1)
[6](https://devops.com/cognition-labs-previews-devin-ai-software-engineer/)
[7](https://demo.topy.ai/cursors-journey-to-300m-arr-a-case-study-on-ai-driven-growth-and-innovation/)
[8](https://www.anthropic.com/solutions/coding?hubs_content=blog.hubspot.com%2525252Fsales&hubs_content-cta=null&partner_key=JeanFran%252525252525C3%252525252525A7oisTOUREL&hubs_post-cta=blognavcard-marketing)
[9](https://www.youtube.com/watch?v=BstJlTJ0tR4)
[10](https://startupstudio.dev/blog/cursor-ais-funding-journey-a-case-study-in-startup-growth-and-ai-innovation)
[11](https://sacra.com/q/how-did-runway-achieve-initial-product-market-fit-and-what-are-the-current-measurable-milestones-in-that-regard/)
[12](https://www.heygen.com/pricing)
[13](https://reelmind.ai/blog/pika-labs-videos-the-future-of-ai-video-generation)
[14](https://research.contrary.com/company/runway)
[15](https://www.heygen.com/blog/how-to-go-viral-with-heygen-ai-video-solutions-2025)
[16](https://reelmind.ai/blog/pika-labs-the-next-wave-of-ai-video-generation)
[17](https://www.youtube.com/watch?v=ZhDX07Mgzyk)
[18](https://www.heygen.com/news)
[19](https://reelmind.ai/blog/pika-labs-generative-ai-experience-next-gen-ai-video)
[20](https://runwayml.com/product/use-cases)


